# üì¶ GUIA: PROCESSAR M√öLTIPLOS FICHEIROS .DAT

## üéØ Objetivo

Este guia explica como processar **centenas ou milhares** de ficheiros .dat automaticamente usando o script `batch_analysis.py`.

---

## ‚úÖ PREPARA√á√ÉO

### 1. Estrutura dos Dados

Coloca todos os ficheiros `.dat` numa pasta, por exemplo:

```
/uploads/
‚îú‚îÄ‚îÄ s0m0.dat
‚îú‚îÄ‚îÄ s0m1.dat
‚îú‚îÄ‚îÄ s0m2.dat
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ s999m3.dat
```

### 2. Instalar Depend√™ncias

```bash
pip install gurobipy matplotlib pandas numpy
```

---

## üöÄ COMO USAR

### Op√ß√£o 1: Teste R√°pido (5-10 ficheiros) ‚≠ê RECOMENDADO

```python
from batch_analysis import BatchAnalyzer

# Criar analisador
analyzer = BatchAnalyzer(
    data_directory='/uploads',
    output_directory='/outputs'
)

# Processar apenas 10 ficheiros para teste
analyzer.run_batch_analysis(
    pattern='*.dat',
    max_files=10,           # Apenas 10 ficheiros
    methods=['bb', 'sa', 'hybrid'],  # Excluir Tabu (√© lento)
    lambda1=0.5,
    lambda2=0.5,
    time_limit=120          # 2 minutos por m√©todo
)
```

**Tempo estimado**: ~5-10 minutos para 10 ficheiros

---

### Op√ß√£o 2: Filtrar por Padr√£o

```python
# Processar apenas ficheiros com M=0
analyzer.run_batch_analysis(
    pattern='*m0.dat',      # Apenas ficheiros que terminam em m0.dat
    max_files=None,         # Todos os que correspondem ao padr√£o
    methods=['bb', 'sa', 'hybrid'],
    lambda1=0.5,
    lambda2=0.5,
    time_limit=120
)
```

**Padr√µes √∫teis**:
- `'s0*.dat'` - Todos os ficheiros que come√ßam com s0
- `'*m0.dat'` - Todos com M=0
- `'*m1.dat'` - Todos com M=1
- `'*.dat'` - TODOS os ficheiros

---

### Op√ß√£o 3: Processar TODOS os 1000 Ficheiros ‚ö†Ô∏è

```python
analyzer.run_batch_analysis(
    pattern='*.dat',
    max_files=None,         # SEM LIMITE!
    methods=['bb', 'sa', 'hybrid'],
    lambda1=0.5,
    lambda2=0.5,
    time_limit=300          # 5 minutos por m√©todo
)
```

**‚ö†Ô∏è ATEN√á√ÉO**:
- Tempo estimado: **5-20 HORAS** (depende do hardware)
- Recomenda-se executar de noite ou em servidor
- Deixa a correr e vai fazer outra coisa!

---

## ‚öôÔ∏è CONFIGURA√á√ïES

### Escolher M√©todos

```python
methods=['bb', 'sa', 'hybrid']  # Recomendado (r√°pido + completo)
methods=['bb']                   # Apenas Branch & Bound
methods=['sa', 'tabu']           # Apenas metaheur√≠sticas
methods=['bb', 'sa', 'tabu', 'hybrid']  # TODOS (lento!)
```

**Recomenda√ß√£o**: 
- **Testes**: `['bb', 'sa', 'hybrid']` (exclui Tabu que √© lento)
- **An√°lise completa**: `['bb', 'sa', 'tabu', 'hybrid']`

### Ajustar Tempo Limite

```python
time_limit=60    # 1 minuto (r√°pido, mas pode n√£o resolver problemas grandes)
time_limit=120   # 2 minutos (bom compromisso)
time_limit=300   # 5 minutos (para problemas dif√≠ceis)
time_limit=600   # 10 minutos (an√°lise profunda)
```

### Variar Pesos

```python
# Priorizar custo operacional
lambda1=0.8, lambda2=0.2

# Priorizar equil√≠brio de carga
lambda1=0.2, lambda2=0.8

# Balanceado
lambda1=0.5, lambda2=0.5
```

---

## üìä OUTPUTS GERADOS

Ap√≥s a execu√ß√£o, o script gera **4 ficheiros**:

### 1. `batch_results.csv`
Tabela com todos os resultados (pode abrir no Excel):

| filename | num_patients | bb_time | bb_objective | sa_time | sa_objective | ... |
|----------|-------------|---------|--------------|---------|--------------|-----|
| s0m0.dat | 117 | 0.01 | 5380.91 | 0.17 | 5819.41 | ... |
| s1m0.dat | 125 | 0.02 | 6234.56 | 0.18 | 6521.32 | ... |

### 2. `batch_comparison.png`
Gr√°fico com boxplots comparando:
- Distribui√ß√£o dos tempos de execu√ß√£o
- Distribui√ß√£o dos desvios do √≥timo

### 3. `scalability.png`
Gr√°fico scatter mostrando:
- Como o tempo aumenta com o tamanho do problema
- Compara√ß√£o entre m√©todos

### 4. `batch_report.txt`
Relat√≥rio textual com estat√≠sticas agregadas:
- Tempo m√©dio/mediano/desvio padr√£o por m√©todo
- Desvio m√©dio do √≥timo
- N√∫mero de inst√¢ncias resolvidas otimamente

---

## üìà AN√ÅLISE DOS RESULTADOS

### Abrir o CSV no Python

```python
import pandas as pd

df = pd.read_csv('/outputs/batch_results.csv')

# Ver resumo
print(df.describe())

# Ver inst√¢ncias mais dif√≠ceis
print(df.nlargest(10, 'bb_time'))

# Comparar m√©todos
print(df[['bb_time', 'sa_time', 'hybrid_time']].mean())
```

### An√°lises √öteis

```python
# Quantas inst√¢ncias o h√≠brido resolveu otimamente?
optimal_count = (df['hybrid_deviation'].abs() < 0.01).sum()
print(f"H√≠brido √≥timo em {optimal_count}/{len(df)} inst√¢ncias")

# Qual m√©todo √© mais r√°pido em m√©dia?
print(df[['bb_time', 'sa_time', 'hybrid_time']].mean())

# Efici√™ncia do h√≠brido vs B&B
speedup = df['bb_time'] / df['hybrid_time']
print(f"H√≠brido √© {speedup.mean():.2f}√ó a velocidade de B&B")
```

---

## ‚è±Ô∏è ESTIMATIVAS DE TEMPO

### Por Inst√¢ncia (117 pacientes, 4 enfermarias):

| M√©todo | Tempo M√©dio | Notas |
|--------|-------------|-------|
| Branch & Bound | ~0.01s | Muito r√°pido para pequenos problemas |
| Simulated Annealing | ~0.17s | Consistente |
| Tabu Search | ~22s | LENTO! Evitar para muitas inst√¢ncias |
| H√≠brido | ~0.19s | Ligeiramente mais lento que B&B |

### Para 1000 Ficheiros:

| Configura√ß√£o | Tempo Estimado |
|-------------|----------------|
| `['bb', 'sa', 'hybrid']` | **3-6 horas** |
| `['bb', 'sa']` | **2-4 horas** |
| `['bb', 'sa', 'tabu', 'hybrid']` | **10-20 horas** ‚ö†Ô∏è |

**Dica**: Come√ßa com 10-50 ficheiros para estimar o tempo real!

---

## üí° ESTRAT√âGIAS RECOMENDADAS

### Estrat√©gia 1: Amostragem (R√ÅPIDO)

```python
# Processar 50 ficheiros aleat√≥rios
import random
analyzer.run_batch_analysis(
    pattern='*.dat',
    max_files=50,
    methods=['bb', 'sa', 'hybrid'],
    time_limit=120
)
```

**Vantagens**:
- R√°pido (~30-45 minutos)
- D√° boa ideia dos resultados gerais
- Suficiente para relat√≥rio

### Estrat√©gia 2: Por Grupo de M (ORGANIZADO)

```python
# Processar cada grupo separadamente
for m in [0, 1, 2, 3]:
    print(f"\n{'='*60}")
    print(f"Processando inst√¢ncias com M={m}")
    print(f"{'='*60}")
    
    analyzer = BatchAnalyzer('/uploads', 
                            f'/outputs/M{m}')
    
    analyzer.run_batch_analysis(
        pattern=f'*m{m}.dat',
        max_files=None,  # Todos deste grupo
        methods=['bb', 'sa', 'hybrid'],
        time_limit=120
    )
```

**Vantagens**:
- Organizado por categoria
- Pode correr cada grupo separadamente
- F√°cil de analisar padr√µes

### Estrat√©gia 3: Progressiva (SEGURA)

```python
# Come√ßar com poucos, aumentar gradualmente
for batch_size in [10, 25, 50, 100]:
    print(f"\nProcessando {batch_size} ficheiros...")
    
    analyzer.run_batch_analysis(
        pattern='*.dat',
        max_files=batch_size,
        methods=['bb', 'sa', 'hybrid'],
        time_limit=120
    )
    
    # Verificar resultados antes de continuar
    input("Pressiona Enter para continuar...")
```

**Vantagens**:
- Seguro (pode parar a qualquer momento)
- Permite ajustar par√¢metros entre batches
- Controlo total

---

## üêõ RESOLU√á√ÉO DE PROBLEMAS

### Problema: "Demasiado lento!"

**Solu√ß√£o 1**: Reduzir m√©todos
```python
methods=['bb', 'hybrid']  # Excluir SA e Tabu
```

**Solu√ß√£o 2**: Reduzir time_limit
```python
time_limit=60  # Apenas 1 minuto
```

**Solu√ß√£o 3**: Processar menos ficheiros
```python
max_files=50  # Amostra
```

### Problema: "Gurobi timeout em muitas inst√¢ncias"

**Solu√ß√£o**: Aumentar time_limit
```python
time_limit=600  # 10 minutos
```

### Problema: "Script crashou a meio"

**Solu√ß√£o**: Os resultados s√£o salvos progressivamente
- Verifica `batch_results.csv` - tem resultados parciais
- Podes retomar processando apenas ficheiros restantes

### Problema: "Mem√≥ria insuficiente"

**Solu√ß√£o**: Processar em batches menores
```python
# Processar 100 de cada vez
for i in range(0, 1000, 100):
    analyzer.run_batch_analysis(
        max_files=100,
        # ...
    )
```

---

## üìä EXEMPLO DE AN√ÅLISE COMPLETA

```python
from batch_analysis import BatchAnalyzer

# Configura√ß√£o
analyzer = BatchAnalyzer(
    data_directory='/uploads',
    output_directory='/outputs'
)

# Processar
analyzer.run_batch_analysis(
    pattern='*.dat',
    max_files=100,          # 100 inst√¢ncias para an√°lise robusta
    methods=['bb', 'sa', 'hybrid'],  # M√©todos principais
    lambda1=0.5,
    lambda2=0.5,
    time_limit=180          # 3 minutos
)

# Analisar resultados
import pandas as pd

df = pd.read_csv('/outputs/batch_results.csv')

print("="*60)
print("RESUMO DA AN√ÅLISE")
print("="*60)

print(f"\nInst√¢ncias processadas: {len(df)}")

print("\nüìä Tempos m√©dios:")
print(f"  B&B:     {df['bb_time'].mean():.2f}s")
print(f"  SA:      {df['sa_time'].mean():.2f}s")
print(f"  H√≠brido: {df['hybrid_time'].mean():.2f}s")

print("\nüéØ Qualidade:")
print(f"  SA desvio m√©dio:      {df['sa_deviation'].mean():.2f}%")
print(f"  H√≠brido desvio m√©dio: {df['hybrid_deviation'].mean():.2f}%")

print("\n‚≠ê H√≠brido √≥timo:")
optimal = (df['hybrid_deviation'].abs() < 0.01).sum()
print(f"  {optimal}/{len(df)} inst√¢ncias ({optimal/len(df)*100:.1f}%)")
```

---

## üéØ RECOMENDA√á√ÉO FINAL

Para o trabalho acad√©mico, sugiro:

1. **Teste inicial**: 10-20 ficheiros (verificar que funciona)
2. **An√°lise principal**: 50-100 ficheiros (estatisticamente relevante)
3. **Opcional**: 1000 ficheiros completos (se tiveres tempo e poder computacional)

**50-100 inst√¢ncias √© suficiente** para:
- ‚úÖ Demonstrar efic√°cia dos m√©todos
- ‚úÖ Ter estat√≠sticas robustas
- ‚úÖ Identificar padr√µes
- ‚úÖ Completar em tempo razo√°vel (~1-2 horas)

---

## üìû SUPORTE

Se tiveres d√∫vidas:
1. Come√ßa com `max_files=5` para testar
2. Verifica os outputs gerados
3. Ajusta par√¢metros conforme necess√°rio

