# ğŸ¯ Resumo

**Sistema completo de anÃ¡lise em batch** que pode processar **centenas ou milhares** de ficheiros `.dat` automaticamente!

---

## ğŸ“¦ NOVOS FICHEIROS CRIADOS

### 1. [batch_analysis.py] â­
   - Script principal para processamento em batch
   - Processa mÃºltiplos ficheiros automaticamente
   - Gera relatÃ³rios e visualizaÃ§Ãµes agregadas
   - ~500 linhas de cÃ³digo

### 2. [run_batch.py] ğŸš€
   - Script SIMPLES e FÃCIL de usar
   - Apenas configura parÃ¢metros no topo e executa
   - Perfeito para comeÃ§ar rapidamente

### 3. [BATCH_GUIDE.md] ğŸ“š
   - Guia completo de como usar
   - EstratÃ©gias diferentes para diferentes necessidades
   - Exemplos prÃ¡ticos
   - ResoluÃ§Ã£o de problemas

---

## ğŸš€ COMO USAR (3 PASSOS)

### Passo 1: Preparar os Dados

Coloca todos os teus ficheiros `.dat` numa pasta:

```
/uploads/
â”œâ”€â”€ s0m0.dat
â”œâ”€â”€ s0m1.dat
â”œâ”€â”€ s0m2.dat
â”œâ”€â”€ ...
â””â”€â”€ s999m3.dat  (1000 ficheiros)
```

### Passo 2: Executar

**OpÃ§Ã£o A - Script Simples** (Recomendado):

```bash
python3 run_batch.py
```

Edita primeiro os parÃ¢metros no topo do ficheiro:
```python
MAX_FILES = 10        # ComeÃ§a com poucos!
METHODS = ['bb', 'sa', 'hybrid']
TIME_LIMIT = 120
```

**OpÃ§Ã£o B - Python Diretamente**:

```python
from batch_analysis import BatchAnalyzer

analyzer = BatchAnalyzer('/uploads', '/outputs')

analyzer.run_batch_analysis(
    pattern='*.dat',
    max_files=50,  # Ajusta conforme necessÃ¡rio
    methods=['bb', 'sa', 'hybrid'],
    lambda1=0.5,
    lambda2=0.5,
    time_limit=120
)
```

### Passo 3: Analisar Resultados

O sistema gera **4 ficheiros**:

1. **batch_results.csv** - Tabela Excel com todos os resultados
2. **batch_comparison.png** - GrÃ¡ficos comparativos
3. **scalability.png** - AnÃ¡lise de escalabilidade
4. **batch_report.txt** - RelatÃ³rio estatÃ­stico

---

## â±ï¸ ESTIMATIVAS DE TEMPO

### Por Ficheiro (mÃ©dio):
- Branch & Bound: ~0.01s
- Simulated Annealing: ~0.17s
- HÃ­brido: ~0.19s
- **Total por ficheiro**: ~0.4s

### Para Diferentes Quantidades:

| Ficheiros | MÃ©todos | Tempo Estimado |
|-----------|---------|----------------|
| 10 | bb + sa + hybrid | ~5 minutos |
| 50 | bb + sa + hybrid | ~30 minutos |
| 100 | bb + sa + hybrid | ~1 hora |
| 1000 | bb + sa + hybrid | **~10 horas** |

**ğŸ’¡ RecomendaÃ§Ã£o**: ComeÃ§a com 50-100 ficheiros para anÃ¡lise robusta sem demorar muito!

---

## ğŸ¯ ESTRATÃ‰GIAS RECOMENDADAS

### 1ï¸âƒ£ Teste RÃ¡pido (5-10 min)
```python
max_files=10
methods=['bb', 'sa', 'hybrid']
time_limit=120
```
âœ… Para verificar que tudo funciona

### 2ï¸âƒ£ AnÃ¡lise Robusta (1-2 horas)
```python
max_files=100
methods=['bb', 'sa', 'hybrid']
time_limit=180
```
âœ… Estatisticamente significativo
âœ… Suficiente para o trabalho acadÃ©mico

### 3ï¸âƒ£ AnÃ¡lise Completa (10-20 horas)
```python
max_files=None  # TODOS!
methods=['bb', 'sa', 'hybrid']
time_limit=300
```
âœ… AnÃ¡lise exaustiva
âš ï¸ Demora muito (deixa a correr de noite)

### 4ï¸âƒ£ Por Grupos de M
```python
# Processar cada M separadamente
for m in [0, 1, 2, 3]:
    analyzer.run_batch_analysis(
        pattern=f'*m{m}.dat',
        max_files=None,
        ...
    )
```
âœ… Organizado
âœ… Permite analisar impacto de M

---

## ğŸ“Š O QUE PODES ANALISAR

Com os resultados do batch, podes responder:

âœ… **Desempenho mÃ©dio** de cada mÃ©todo  
âœ… **Escalabilidade**: Como o tempo aumenta com o tamanho?  
âœ… **Robustez**: Quantas instÃ¢ncias cada mÃ©todo resolve otimamente?  
âœ… **Trade-offs**: Tempo vs Qualidade  
âœ… **Impacto de M**: Como especializaÃ§Ãµes menores afetam dificuldade?  
âœ… **Casos difÃ­ceis**: Quais instÃ¢ncias sÃ£o mais desafiantes?  

---

## ğŸ“ˆ EXEMPLO DE ANÃLISE

```python
import pandas as pd

# Carregar resultados
df = pd.read_csv('batch_results.csv')

# EstatÃ­sticas bÃ¡sicas
print("Tempo mÃ©dio por mÃ©todo:")
print(df[['bb_time', 'sa_time', 'hybrid_time']].mean())

# HÃ­brido vs B&B
print("\nHÃ­brido conseguiu Ã³timo em:")
optimal = (df['hybrid_deviation'].abs() < 0.01).sum()
print(f"{optimal}/{len(df)} instÃ¢ncias ({optimal/len(df)*100:.1f}%)")

# Identificar casos difÃ­ceis
print("\n5 instÃ¢ncias mais difÃ­ceis:")
print(df.nlargest(5, 'bb_time')[['filename', 'num_patients', 'bb_time']])

# AnÃ¡lise por M
print("\nTempo mÃ©dio por valor de M:")
print(df.groupby('M')['bb_time'].mean())
```

---

## ğŸ“ PARA O RELATÃ“RIO

Com este sistema podes:

1. **Tabela Comparativa**
   ```
   Usar dados de batch_results.csv
   Mostrar mÃ©dia, mediana, desvio padrÃ£o
   ```

2. **GrÃ¡ficos Profissionais**
   ```
   Usar batch_comparison.png e scalability.png
   GrÃ¡ficos prontos para slides!
   ```

3. **AnÃ¡lise EstatÃ­stica**
   ```
   Teste t, intervalos de confianÃ§a
   CorrelaÃ§Ãµes entre variÃ¡veis
   ```

4. **ConclusÃµes Robustas**
   ```
   Baseadas em dezenas/centenas de instÃ¢ncias
   NÃ£o apenas 1 exemplo!
   ```

---

## âš ï¸ PONTOS IMPORTANTES

### âœ… FAZER:
- ComeÃ§ar com **poucos ficheiros** (5-10) para testar
- Usar `methods=['bb', 'sa', 'hybrid']` (excluir Tabu que Ã© lento)
- Monitorizar o progresso (imprime status)
- Guardar resultados incrementalmente (CSV Ã© atualizado)

### âŒ NÃƒO FAZER:
- ComeÃ§ar diretamente com 1000 ficheiros
- Incluir Tabu se tiveres muitos ficheiros (demora MUITO)
- Deixar time_limit muito alto (>10min) sem necessidade
- Esquecer de verificar o espaÃ§o em disco

---

## ğŸ†˜ RESOLUÃ‡ÃƒO DE PROBLEMAS

### "Muito lento!"
â†’ Reduz `max_files` ou `time_limit`  
â†’ Exclui Tabu dos mÃ©todos

### "Ficou sem memÃ³ria"
â†’ Processa em batches menores (50-100 de cada vez)

### "Gurobi timeout"
â†’ Aumenta `time_limit`  
â†’ Algumas instÃ¢ncias podem ser muito difÃ­ceis

### "Script parou a meio"
â†’ Verifica `batch_results.csv` (tem resultados parciais)  
â†’ Podes retomar excluindo ficheiros jÃ¡ processados

---

## ğŸ‰ RESUMO FINAL

### âœ… O que tens agora:

1. **Sistema completo** para processar 1000+ ficheiros
2. **AutomÃ¡tico** - configura e deixa correr
3. **AnÃ¡lise agregada** - estatÃ­sticas de todos os ficheiros
4. **VisualizaÃ§Ãµes** - grÃ¡ficos profissionais
5. **RelatÃ³rios** - prontos para o trabalho

### ğŸ¯ RecomendaÃ§Ã£o Final:

Para o trabalho acadÃ©mico:
1. **Teste**: 10 ficheiros (verificar)
2. **AnÃ¡lise principal**: 50-100 ficheiros (robusto)
3. **Opcional**: 1000 ficheiros (se tiveres tempo)

**50-100 ficheiros Ã© mais do que suficiente** para:
- âœ… Demonstrar eficÃ¡cia
- âœ… Ter estatÃ­sticas robustas
- âœ… Completar em tempo razoÃ¡vel (~1-2 horas)

---

## ğŸ“ PRÃ“XIMOS PASSOS

1. **LÃª**: [BATCH_GUIDE.md](computer:///mnt/user-data/outputs/BATCH_GUIDE.md)
2. **Testa**: Executa com `max_files=5`
3. **Ajusta**: Muda parÃ¢metros conforme necessÃ¡rio
4. **Executa**: AnÃ¡lise completa (50-100 ficheiros)
5. **Analisa**: Usa os CSVs e grÃ¡ficos gerados

---

DÃºvida? Consulta o BATCH_GUIDE.md que tem tudo explicado em detalhe!
